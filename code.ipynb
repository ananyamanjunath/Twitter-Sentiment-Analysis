{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Downloading NLTK resources (only need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Loading the training and validation data\n",
    "training_data = pd.read_csv('twitter_training.csv')\n",
    "validation_data = pd.read_csv('twitter_validation.csv')\n",
    "training_data['text'] = training_data['im getting on borderlands and i will murder you all ,']\n",
    "validation_data['text'] = validation_data['I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£']\n",
    "\n",
    "training_data.drop('im getting on borderlands and i will murder you all ,', axis=1, inplace=True)\n",
    "validation_data.drop('I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£',axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Handling NaN values\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing special characters, URLs, and mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocessing the text data\n",
    "training_data['text'] = training_data['text'].apply(preprocess_text)\n",
    "validation_data['text'] = validation_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Development</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.963963963963964\n"
     ]
    }
   ],
   "source": [
    "# Initializing TF-IDF vectorizer with adjusted parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')  # Adjust max_features and stopwords\n",
    "\n",
    "# Fitting and transforming the text data\n",
    "X_train = tfidf_vectorizer.fit_transform(training_data['text'])\n",
    "X_val = tfidf_vectorizer.transform(validation_data['text'])\n",
    "\n",
    "# Encoding sentiment labels\n",
    "sentiment_mapping = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'Irrelevant': 3}\n",
    "training_data['sentiment'] = training_data['Positive'].map(sentiment_mapping)\n",
    "validation_data['sentiment'] = validation_data['Irrelevant'].map(sentiment_mapping)\n",
    "\n",
    "y_train = training_data['sentiment']\n",
    "y_val = validation_data['sentiment']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on validation data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
